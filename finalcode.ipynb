{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09136f-81da-4ac4-8dfb-1867fbd98fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_lightning-2.4.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/torchmetrics-1.5.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabnet-4.1.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/einops-0.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/download-lightning-and-pytorch-tabular/pytorch_tabular-1.1.1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ef1514-e935-4d79-a203-083910e8c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from metric import score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "ROOT_DATA_PATH = Path(r\"/kaggle/input/equity-post-HCT-survival-predictions\")\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "train = pd.read_csv(ROOT_DATA_PATH.joinpath(\"train.csv\"))\n",
    "test = pd.read_csv(ROOT_DATA_PATH.joinpath(\"test.csv\"))\n",
    "\n",
    "CATEGORICAL_VARIABLES = [\n",
    "    # Graft and HCT reasons\n",
    "    'dri_score', 'graft_type', 'prod_type', 'prim_disease_hct',\n",
    "\n",
    "    # Patient health status (risk factors)\n",
    "    'psych_disturb', 'diabetes', 'arrhythmia', 'vent_hist', 'renal_issue', 'pulm_moderate',\n",
    "    'pulm_severe', 'obesity', 'hepatic_mild', 'hepatic_severe', 'peptic_ulcer', 'rheum_issue',\n",
    "    'cardiac', 'prior_tumor', 'mrd_hct', 'tbi_status', 'cyto_score', 'cyto_score_detail', \n",
    "\n",
    "    # Patient demographics\n",
    "    'ethnicity', 'race_group',\n",
    "\n",
    "    # Biological matching with donor\n",
    "    'sex_match', 'donor_related', 'cmv_status', 'tce_imm_match', 'tce_match', 'tce_div_match',\n",
    "\n",
    "    # Medication/operation related data\n",
    "    'melphalan_dose', 'rituximab', 'gvhd_proph', 'in_vivo_tcd', 'conditioning_intensity'\n",
    "]\n",
    "\n",
    "HLA_COLUMNS = [\n",
    "    'hla_match_a_low', 'hla_match_a_high',\n",
    "    'hla_match_b_low', 'hla_match_b_high',\n",
    "    'hla_match_c_low', 'hla_match_c_high',\n",
    "    'hla_match_dqb1_low', 'hla_match_dqb1_high',\n",
    "    'hla_match_drb1_low', 'hla_match_drb1_high',\n",
    "    \n",
    "    # Matching at HLA-A(low), -B(low), -DRB1(high)\n",
    "    'hla_nmdp_6',\n",
    "    # Matching at HLA-A,-B,-DRB1 (low or high)\n",
    "    'hla_low_res_6', 'hla_high_res_6',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1 (low or high)\n",
    "    'hla_low_res_8', 'hla_high_res_8',\n",
    "    # Matching at HLA-A, -B, -C, -DRB1, -DQB1 (low or high)\n",
    "    'hla_low_res_10', 'hla_high_res_10'\n",
    "]\n",
    "\n",
    "OTHER_NUMERICAL_VARIABLES = ['year_hct', 'donor_age', 'age_at_hct', 'comorbidity_score', 'karnofsky_score']\n",
    "NUMERICAL_VARIABLES = HLA_COLUMNS + OTHER_NUMERICAL_VARIABLES\n",
    "\n",
    "TARGET_VARIABLES = ['efs_time', 'efs']\n",
    "ID_COLUMN = [\"ID\"]\n",
    "\n",
    "\n",
    "def preprocess_data(df):\n",
    "    df[CATEGORICAL_VARIABLES] = df[CATEGORICAL_VARIABLES].fillna(\"Unknown\")\n",
    "    df[OTHER_NUMERICAL_VARIABLES] = df[OTHER_NUMERICAL_VARIABLES].fillna(df[OTHER_NUMERICAL_VARIABLES].median())\n",
    "\n",
    "    return df\n",
    "\n",
    "train = preprocess_data(train)\n",
    "test = preprocess_data(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c66ec-c978-4a98-8780-58e6b60333e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    # Change year_hct to relative year from 2000\n",
    "    df['year_hct'] = df['year_hct'] - 2000\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "train = features_engineering(train)\n",
    "test = features_engineering(test)\n",
    "\n",
    "train[CATEGORICAL_VARIABLES] = train[CATEGORICAL_VARIABLES].astype('category')\n",
    "test[CATEGORICAL_VARIABLES] = test[CATEGORICAL_VARIABLES].astype('category')\n",
    "\n",
    "FEATURES = train.drop(columns=['ID', 'efs', 'efs_time']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ab10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "import xgboost\n",
    "print(\"Using XGBoost version\",xgboost.__version__)\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "FOLDS = 5\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_xgb = np.zeros(len(train))\n",
    "pred_efs = np.zeros(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09b666-4654-4b44-86cf-d79532078586",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train, train[\"efs\"])):\n",
    "\n",
    "    print(\"#\"*25)\n",
    "    print(f\"### Fold {i+1}\")\n",
    "    print(\"#\"*25)\n",
    "    \n",
    "    x_train = train.loc[train_index, FEATURES].copy()\n",
    "    y_train = train.loc[train_index, \"efs\"]\n",
    "    x_valid = train.loc[test_index, FEATURES].copy()\n",
    "    y_valid = train.loc[test_index, \"efs\"]\n",
    "    x_test = test[FEATURES].copy()\n",
    "\n",
    "    model_xgb = XGBClassifier(\n",
    "        device=\"cuda\",\n",
    "        max_depth=3,  \n",
    "        colsample_bytree=0.7129400756425178, \n",
    "        subsample=0.8185881823156917, \n",
    "        n_estimators=20_000, \n",
    "        learning_rate=0.04425768131771064,  \n",
    "        eval_metric=\"auc\", \n",
    "        early_stopping_rounds=50, \n",
    "        objective='binary:logistic',\n",
    "        scale_pos_weight=1.5379160847615545,  \n",
    "        min_child_weight=4,\n",
    "        enable_categorical=True,\n",
    "        gamma=3.1330719334577584\n",
    "    )\n",
    "    model_xgb.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],  \n",
    "        verbose=100\n",
    "    )\n",
    "\n",
    "    # INFER OOF (Probabilities -> Binary)\n",
    "    oof_xgb[test_index] = model_xgb.predict_proba(x_valid)[:, 1] \n",
    "    # INFER TEST (Probabilities -> Average Probs)\n",
    "    pred_efs += model_xgb.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# COMPUTE AVERAGE TEST PREDS\n",
    "pred_efs = pred_efs / FOLDS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe493b0-4ce2-4455-be43-ffa259e7ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EVALUATE PERFORMANCE\n",
    "accuracy = accuracy_score(train[\"efs\"], oof_xgb> 0.5)\n",
    "f1 = f1_score(train[\"efs\"], oof_xgb> 0.5)\n",
    "roc_auc = roc_auc_score(train[\"efs\"], oof_xgb> 0.5)\n",
    "oof_xgb_clf = oof_xgb.copy()\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.4f}\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_X_cat(df, cat_cols, transformers=None):\n",
    "    \"\"\"\n",
    "    Apply a specific categorical data transformer or a LabelEncoder if None.\n",
    "    \"\"\"\n",
    "    if transformers is None:\n",
    "        transformers = [LabelEncoder().fit(df[col]) for col in cat_cols]\n",
    "    return transformers, np.array(\n",
    "        [transformer.transform(df[col]) for col, transformer in zip(cat_cols, transformers)]\n",
    "    ).T\n",
    "\n",
    "\n",
    "def preprocess_data(train, val):\n",
    "    \"\"\"\n",
    "    Standardize numerical variables and transform (Label-encode) categoricals.\n",
    "    Fill NA values with mean for numerical.\n",
    "    Create torch dataloaders to prepare data for training and evaluation.\n",
    "    \"\"\"\n",
    "    X_cat_train, X_cat_val, numerical, transformers = get_categoricals(train, val)\n",
    "    scaler = StandardScaler()\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean', add_indicator=True)\n",
    "    X_num_train = imp.fit_transform(train[numerical])\n",
    "    X_num_train = scaler.fit_transform(X_num_train)\n",
    "    X_num_val = imp.transform(val[numerical])\n",
    "    X_num_val = scaler.transform(X_num_val)\n",
    "    dl_train = init_dl(X_cat_train, X_num_train, train, training=True)\n",
    "    dl_val = init_dl(X_cat_val, X_num_val, val)\n",
    "    return X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers\n",
    "\n",
    "\n",
    "def get_categoricals(train, val):\n",
    "    \"\"\"\n",
    "    Remove constant categorical columns and transform them using LabelEncoder.\n",
    "    Return the label-transformers for each categorical column, categorical dataframes and numerical columns.\n",
    "    \"\"\"\n",
    "    categorical_cols, numerical = get_feature_types(train)\n",
    "    remove = []\n",
    "    for col in categorical_cols:\n",
    "        if train[col].nunique() == 1:\n",
    "            remove.append(col)\n",
    "        ind = ~val[col].isin(train[col])\n",
    "        if ind.any():\n",
    "            val.loc[ind, col] = np.nan\n",
    "    categorical_cols = [col for col in categorical_cols if col not in remove]\n",
    "    transformers, X_cat_train = get_X_cat(train, categorical_cols)\n",
    "    _, X_cat_val = get_X_cat(val, categorical_cols, transformers)\n",
    "    return X_cat_train, X_cat_val, numerical, transformers\n",
    "\n",
    "\n",
    "def init_dl(X_cat, X_num, df, training=False):\n",
    "    \"\"\"\n",
    "    Initialize data loaders with 4 dimensions : categorical dataframe, numerical dataframe and target values (efs and efs_time).\n",
    "    Notice that efs_time is log-transformed.\n",
    "    Fix batch size to 2048 and return dataloader for training or validation depending on training value.\n",
    "    \"\"\"\n",
    "    ds_train = TensorDataset(\n",
    "        torch.tensor(X_cat, dtype=torch.long),\n",
    "        torch.tensor(X_num, dtype=torch.float32),\n",
    "        torch.tensor(df.efs_time.values, dtype=torch.float32).log(),\n",
    "        torch.tensor(df.efs.values, dtype=torch.long)\n",
    "    )\n",
    "    bs = 2048\n",
    "    dl_train = torch.utils.data.DataLoader(ds_train, batch_size=bs, pin_memory=True, shuffle=training)\n",
    "    return dl_train\n",
    "\n",
    "\n",
    "def get_feature_types(train):\n",
    "    \"\"\"\n",
    "    Utility function to return categorical and numerical column names.\n",
    "    \"\"\"\n",
    "    categorical_cols = [col for i, col in enumerate(train.columns) if ((train[col].dtype == \"object\") | (2 < train[col].nunique() < 25))]\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "    FEATURES = [c for c in train.columns if not c in RMV]\n",
    "    numerical = [i for i in FEATURES if i not in categorical_cols]\n",
    "    return categorical_cols, numerical\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"\n",
    "    Create some new features to help the model focus on specific patterns.\n",
    "    \"\"\"\n",
    "    # sex_match = df.sex_match.astype(str)\n",
    "    # sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n",
    "    # df['sex_match_bool'] = sex_match\n",
    "    # df.loc[df.sex_match.isna(), 'sex_match_bool'] = np.nan\n",
    "    # df['big_age'] = df.age_at_hct > 16\n",
    "    # df.loc[df.year_hct == 2019, 'year_hct'] = 2020\n",
    "    df['is_cyto_score_same'] = (df['cyto_score'] == df['cyto_score_detail']).astype(int)\n",
    "    # df['strange_age'] = df.age_at_hct == 0.044\n",
    "    # df['age_bin'] = pd.cut(df.age_at_hct, [0, 0.0441, 16, 30, 50, 100])\n",
    "    # df['age_ts'] = df.age_at_hct / df.donor_age\n",
    "    df['year_hct'] -= 2000\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load data and add features.\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    test = add_features(test)\n",
    "    print(\"Test shape:\", test.shape)\n",
    "    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "    train = add_features(train)\n",
    "    print(\"Train shape:\", train.shape)\n",
    "    return test, train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cba31d-a24a-41a3-8d2b-4b2c69fb353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from typing import List\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from lifelines.utils import concordance_index\n",
    "from pytorch_lightning.cli import ReduceLROnPlateau\n",
    "from pytorch_tabular.models.common.layers import ODST\n",
    "from torch import nn\n",
    "from pytorch_lightning.utilities import grad_norm\n",
    "\n",
    "\n",
    "class CatEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding module for the categorical dataframe.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        projection_dim: int,\n",
    "        categorical_cardinality: List[int],\n",
    "        embedding_dim: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        projection_dim: The dimension of the final output after projecting the concatenated embeddings into a lower-dimensional space.\n",
    "        categorical_cardinality: A list where each element represents the number of unique categories (cardinality) in each categorical feature.\n",
    "        embedding_dim: The size of the embedding space for each categorical feature.\n",
    "        self.embeddings: list of embedding layers for each categorical feature.\n",
    "        self.projection: sequential neural network that goes from the embedding to the output projection dimension with GELU activation.\n",
    "        \"\"\"\n",
    "        super(CatEmbeddings, self).__init__()\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(cardinality, embedding_dim)\n",
    "            for cardinality in categorical_cardinality\n",
    "        ])\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embedding_dim * len(categorical_cardinality), projection_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(projection_dim, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"\n",
    "        Apply the projection on concatened embeddings that contains all categorical features.\n",
    "        \"\"\"\n",
    "        x_cat = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embeddings)]\n",
    "        x_cat = torch.cat(x_cat, dim=1)\n",
    "        return self.projection(x_cat)\n",
    "\n",
    "\n",
    "class NN(nn.Module):\n",
    "    \"\"\"\n",
    "    Train a model on both categorical embeddings and numerical data.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            dropout: float = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous features.\n",
    "        categorical_cardinality: A list of integers representing the number of unique categories in each categorical feature.\n",
    "        embedding_dim: The dimensionality of the embedding space for each categorical feature.\n",
    "        projection_dim: The size of the projected output space for the categorical embeddings.\n",
    "        hidden_dim: The number of neurons in the hidden layer of the MLP.\n",
    "        dropout: The dropout rate applied in the network.\n",
    "        self.embeddings: previous embeddings for categorical data.\n",
    "        self.mlp: defines an MLP model with an ODST layer followed by batch normalization and dropout.\n",
    "        self.out: linear output layer that maps the output of the MLP to a single value\n",
    "        self.dropout: defines dropout\n",
    "        Weights initialization with xavier normal algorithm and biases with zeros.\n",
    "        \"\"\"\n",
    "        super(NN, self).__init__()\n",
    "        self.embeddings = CatEmbeddings(projection_dim, categorical_cardinality, embedding_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            ODST(projection_dim + continuous_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Create embedding layers for categorical data, concatenate with continous variables.\n",
    "        Add dropout and goes through MLP and return raw output and 1-dimensional output as well.\n",
    "        \"\"\"\n",
    "        x = self.embeddings(x_cat)\n",
    "        x = torch.cat([x, x_cont], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.mlp(x)\n",
    "        return self.out(x), x\n",
    "\n",
    "\n",
    "@functools.lru_cache\n",
    "def combinations(N):\n",
    "    \"\"\"\n",
    "    calculates all possible 2-combinations (pairs) of a tensor of indices from 0 to N-1, \n",
    "    and caches the result using functools.lru_cache for optimization\n",
    "    \"\"\"\n",
    "    ind = torch.arange(N)\n",
    "    comb = torch.combinations(ind, r=2)\n",
    "    return comb.cuda()\n",
    "\n",
    "\n",
    "class LitNN(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Main Model creation and losses definition to fully train the model.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            continuous_dim: int,\n",
    "            categorical_cardinality: List[int],\n",
    "            embedding_dim: int,\n",
    "            projection_dim: int,\n",
    "            hidden_dim: int,\n",
    "            lr: float = 1e-3,\n",
    "            dropout: float = 0.2,\n",
    "            weight_decay: float = 1e-3,\n",
    "            aux_weight: float = 0.1,\n",
    "            margin: float = 0.5,\n",
    "            race_index: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        continuous_dim: The number of continuous input features.\n",
    "        categorical_cardinality: A list of integers, where each element corresponds to the number of unique categories for each categorical feature.\n",
    "        embedding_dim: The dimension of the embeddings for the categorical features.\n",
    "        projection_dim: The dimension of the projected space after embedding concatenation.\n",
    "        hidden_dim: The size of the hidden layers in the feedforward network (MLP).\n",
    "        lr: The learning rate for the optimizer.\n",
    "        dropout: Dropout probability to avoid overfitting.\n",
    "        weight_decay: The L2 regularization term for the optimizer.\n",
    "        aux_weight: Weight used for auxiliary tasks.\n",
    "        margin: Margin used in some loss functions.\n",
    "        race_index: An index that refer to race_group in the input data.\n",
    "        \"\"\"\n",
    "        super(LitNN, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # Creates an instance of the NN model defined above\n",
    "        self.model = NN(\n",
    "            continuous_dim=self.hparams.continuous_dim,\n",
    "            categorical_cardinality=self.hparams.categorical_cardinality,\n",
    "            embedding_dim=self.hparams.embedding_dim,\n",
    "            projection_dim=self.hparams.projection_dim,\n",
    "            hidden_dim=self.hparams.hidden_dim,\n",
    "            dropout=self.hparams.dropout\n",
    "        )\n",
    "        self.targets = []\n",
    "\n",
    "        # Defines a small feedforward neural network that performs an auxiliary task with 1-dimensional output\n",
    "        self.aux_cls = nn.Sequential(\n",
    "            nn.Linear(self.hparams.hidden_dim, self.hparams.hidden_dim // 3),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hparams.hidden_dim // 3, 1)\n",
    "        )\n",
    "\n",
    "    def on_before_optimizer_step(self, optimizer):\n",
    "        \"\"\"\n",
    "        Compute the 2-norm for each layer\n",
    "        If using mixed precision, the gradients are already unscaled here\n",
    "        \"\"\"\n",
    "        norms = grad_norm(self.model, norm_type=2)\n",
    "        self.log_dict(norms)\n",
    "\n",
    "    def forward(self, x_cat, x_cont):\n",
    "        \"\"\"\n",
    "        Forward pass that outputs the 1-dimensional prediction and the embeddings (raw output)\n",
    "        \"\"\"\n",
    "        x, emb = self.model(x_cat, x_cont)\n",
    "        return x.squeeze(1), emb\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        defines how the model processes each batch of data during training.\n",
    "        A batch is a combination of : categorical data, continuous data, efs_time (y) and efs event.\n",
    "        y_hat is the efs_time prediction on all data and aux_pred is auxiliary prediction on embeddings.\n",
    "        Calculates loss and race_group loss on full data.\n",
    "        Auxiliary loss is calculated with an event mask, ignoring efs=0 predictions and taking the average.\n",
    "        Returns loss and aux_loss multiplied by weight defined above.\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        aux_pred = self.aux_cls(emb).squeeze(1)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        aux_loss = nn.functional.mse_loss(aux_pred, y, reduction='none')\n",
    "        aux_mask = efs == 1\n",
    "        aux_loss = (aux_loss * aux_mask).sum() / aux_mask.sum()\n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"race_loss\", race_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        self.log(\"aux_loss\", aux_loss, on_epoch=True, prog_bar=True, logger=True, on_step=False)\n",
    "        return loss + aux_loss * self.hparams.aux_weight\n",
    "\n",
    "    def get_full_loss(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Output loss and race_group loss.\n",
    "        \"\"\"\n",
    "        loss = self.calc_loss(y, y_hat, efs)\n",
    "        race_loss = self.get_race_losses(efs, x_cat, y, y_hat)\n",
    "        loss += 0.1 * race_loss\n",
    "        return loss, race_loss\n",
    "\n",
    "    def get_race_losses(self, efs, x_cat, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate loss for each race_group based on deviation/variance.\n",
    "        \"\"\"\n",
    "        races = torch.unique(x_cat[:, self.hparams.race_index])\n",
    "        race_losses = []\n",
    "        for race in races:\n",
    "            ind = x_cat[:, self.hparams.race_index] == race\n",
    "            race_losses.append(self.calc_loss(y[ind], y_hat[ind], efs[ind]))\n",
    "        race_loss = sum(race_losses) / len(race_losses)\n",
    "        races_loss_std = sum((r - race_loss)**2 for r in race_losses) / len(race_losses)\n",
    "        return torch.sqrt(races_loss_std)\n",
    "\n",
    "    def calc_loss(self, y, y_hat, efs):\n",
    "        \"\"\"\n",
    "        Most important part of the model : loss function used for training.\n",
    "        We face survival data with event indicators along with time-to-event.\n",
    "\n",
    "        This function computes the main loss by the following the steps :\n",
    "        * create all data pairs with \"combinations\" function (= all \"two subjects\" combinations)\n",
    "        * make sure that we have at least 1 event in each pair\n",
    "        * convert y to +1 or -1 depending on the correct ranking\n",
    "        * loss is computed using a margin-based hinge loss\n",
    "        * mask is applied to ensure only valid pairs are being used (censored data can't be ranked with event in some cases)\n",
    "        * average loss on all pairs is returned\n",
    "        \"\"\"\n",
    "        N = y.shape[0]\n",
    "        comb = combinations(N)\n",
    "        comb = comb[(efs[comb[:, 0]] == 1) | (efs[comb[:, 1]] == 1)]\n",
    "        pred_left = y_hat[comb[:, 0]]\n",
    "        pred_right = y_hat[comb[:, 1]]\n",
    "        y_left = y[comb[:, 0]]\n",
    "        y_right = y[comb[:, 1]]\n",
    "        y = 2 * (y_left > y_right).int() - 1\n",
    "        loss = nn.functional.relu(-y * (pred_left - pred_right) + self.hparams.margin)\n",
    "        mask = self.get_mask(comb, efs, y_left, y_right)\n",
    "        loss = (loss.double() * (mask.double())).sum() / mask.sum()\n",
    "        return loss\n",
    "\n",
    "    def get_mask(self, comb, efs, y_left, y_right):\n",
    "        \"\"\"\n",
    "        Defines all invalid comparisons :\n",
    "        * Case 1: \"Left outlived Right\" but Right is censored\n",
    "        * Case 2: \"Right outlived Left\" but Left is censored\n",
    "        Masks for case 1 and case 2 are combined using |= operator and inverted using ~ to create a \"valid pair mask\"\n",
    "        \"\"\"\n",
    "        left_outlived = y_left >= y_right\n",
    "        left_1_right_0 = (efs[comb[:, 0]] == 1) & (efs[comb[:, 1]] == 0)\n",
    "        mask2 = (left_outlived & left_1_right_0)\n",
    "        right_outlived = y_right >= y_left\n",
    "        right_1_left_0 = (efs[comb[:, 1]] == 1) & (efs[comb[:, 0]] == 0)\n",
    "        mask2 |= (right_outlived & right_1_left_0)\n",
    "        mask2 = ~mask2\n",
    "        mask = mask2\n",
    "        return mask\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        This method defines how the model processes each batch during validation\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"\n",
    "        At the end of the validation epoch, it computes and logs the concordance index\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "    def _calc_cindex(self):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group or global.\n",
    "        \"\"\"\n",
    "        y = torch.cat([t[0] for t in self.targets]).cpu().numpy()\n",
    "        y_hat = torch.cat([t[1] for t in self.targets]).cpu().numpy()\n",
    "        efs = torch.cat([t[2] for t in self.targets]).cpu().numpy()\n",
    "        races = torch.cat([t[3] for t in self.targets]).cpu().numpy()\n",
    "        metric = self._metric(efs, races, y, y_hat)\n",
    "        cindex = concordance_index(y, y_hat, efs)\n",
    "        return cindex, metric\n",
    "\n",
    "    def _metric(self, efs, races, y, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate c-index accounting for each race_group\n",
    "        \"\"\"\n",
    "        metric_list = []\n",
    "        for race in np.unique(races):\n",
    "            y_ = y[races == race]\n",
    "            y_hat_ = y_hat[races == race]\n",
    "            efs_ = efs[races == race]\n",
    "            metric_list.append(concordance_index(y_, y_hat_, efs_))\n",
    "        metric = float(np.mean(metric_list) - np.sqrt(np.var(metric_list)))\n",
    "        return metric\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Same as training step but to log test data\n",
    "        \"\"\"\n",
    "        x_cat, x_cont, y, efs = batch\n",
    "        y_hat, emb = self(x_cat, x_cont)\n",
    "        loss, race_loss = self.get_full_loss(efs, x_cat, y, y_hat)\n",
    "        self.targets.append([y, y_hat.detach(), efs, x_cat[:, self.hparams.race_index]])\n",
    "        self.log(\"test_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        \"\"\"\n",
    "        At the end of the test epoch, calculates and logs the concordance index for the test set\n",
    "        \"\"\"\n",
    "        cindex, metric = self._calc_cindex()\n",
    "        self.log(\"test_cindex\", metric, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"test_cindex_simple\", cindex, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.targets.clear()\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        configures the optimizer and learning rate scheduler:\n",
    "        * Optimizer: Adam optimizer with weight decay (L2 regularization).\n",
    "        * Scheduler: Cosine Annealing scheduler, which adjusts the learning rate according to a cosine curve.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)\n",
    "        scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=45,\n",
    "                eta_min=6e-3\n",
    "            ),\n",
    "            \"interval\": \"epoch\",\n",
    "            \"frequency\": 1,\n",
    "            \"strict\": False,\n",
    "        }\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9634fbd-756b-4bde-9624-6aaf1955ea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, TQDMProgressBar\n",
    "from pytorch_lightning.callbacks import StochasticWeightAveraging\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "def main(hparams):\n",
    "    \"\"\"\n",
    "    Main function to train the model.\n",
    "    The steps are as following :\n",
    "    * load data and fill efs and efs time for test data with 1\n",
    "    * initialize pred array with 0\n",
    "    * get categorical and numerical columns\n",
    "    * split the train data on the stratified criterion : race_group * newborns yes/no\n",
    "    * preprocess the fold data (create dataloaders)\n",
    "    * train the model and create final submission output\n",
    "    \"\"\"\n",
    "    test, train_original = load_data()\n",
    "    test['efs_time'] = 1\n",
    "    test['efs'] = 1\n",
    "    oof_nn_pairwise = np.zeros(len(train_original))\n",
    "    test_pred = np.zeros(test.shape[0])\n",
    "    categorical_cols, numerical = get_feature_types(train_original)\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, )\n",
    "    for i, (train_index, test_index) in enumerate(\n",
    "        kf.split(\n",
    "            train_original, train_original.race_group.astype(str) + (train_original.age_at_hct == 0.044).astype(str)\n",
    "        )\n",
    "    ):\n",
    "        tt = train_original.copy()\n",
    "        train = tt.iloc[train_index]\n",
    "        val = tt.iloc[test_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, val)\n",
    "        model = train_final(X_num_train, dl_train, dl_val, transformers, categorical_cols=categorical_cols)\n",
    "        oof_pred, _ = model.cuda().eval()(\n",
    "            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n",
    "            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n",
    "        )\n",
    "        oof_nn_pairwise[test_index] = oof_pred.detach().cpu().numpy()\n",
    "        # Create submission\n",
    "        train = tt.iloc[train_index]\n",
    "        X_cat_val, X_num_train, X_num_val, dl_train, dl_val, transformers = preprocess_data(train, test)\n",
    "        pred, _ = model.cuda().eval()(\n",
    "            torch.tensor(X_cat_val, dtype=torch.long).cuda(),\n",
    "            torch.tensor(X_num_val, dtype=torch.float32).cuda()\n",
    "        )\n",
    "        test_pred += pred.detach().cpu().numpy()\n",
    "        \n",
    "    \n",
    "    return -test_pred, -oof_nn_pairwise\n",
    "\n",
    "\n",
    "\n",
    "def train_final(X_num_train, dl_train, dl_val, transformers, hparams=None, categorical_cols=None):\n",
    "    \"\"\"\n",
    "    Defines model hyperparameters and fit the model.\n",
    "    \"\"\"\n",
    "    if hparams is None:\n",
    "        hparams = {\n",
    "            \"embedding_dim\": 16,\n",
    "            \"projection_dim\": 112,\n",
    "            \"hidden_dim\": 56,\n",
    "            \"lr\": 0.06464861983337984,\n",
    "            \"dropout\": 0.05463240181423116,\n",
    "            \"aux_weight\": 0.26545778308743806,\n",
    "            \"margin\": 0.2588153271003354,\n",
    "            \"weight_decay\": 0.0002773544957610778\n",
    "        }\n",
    "    model = LitNN(\n",
    "        continuous_dim=X_num_train.shape[1],\n",
    "        categorical_cardinality=[len(t.classes_) for t in transformers],\n",
    "        race_index=categorical_cols.index(\"race_group\"),\n",
    "        **hparams\n",
    "    )\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"val_loss\", save_top_k=1)\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='cuda',\n",
    "        max_epochs=60,\n",
    "        log_every_n_steps=6,\n",
    "        callbacks=[\n",
    "            checkpoint_callback,\n",
    "            LearningRateMonitor(logging_interval='epoch'),\n",
    "            TQDMProgressBar(),\n",
    "            StochasticWeightAveraging(swa_lrs=1e-5, swa_epoch_start=45, annealing_epochs=15)\n",
    "        ],\n",
    "    )\n",
    "    trainer.fit(model, dl_train)\n",
    "    trainer.test(model, dl_val)\n",
    "    return model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d51d42c-f65f-4448-97d1-444a1aa83130",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = None\n",
    "pairwise_ranking_pred, pairwise_ranking_oof = main(hparams)\n",
    "\n",
    "y_true = train[[\"ID\",\"efs\",\"efs_time\",\"race_group\"]].copy()\n",
    "y_pred = train[[\"ID\"]].copy()\n",
    "y_pred[\"prediction\"] = pairwise_ranking_oof\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nPairwise ranking NN CV =\",m)\n",
    "\n",
    "# Update predictions with classifier mask\n",
    "pairwise_ranking_oof[oof_xgb_clf >0.45] += 0.1\n",
    "pairwise_ranking_oof[oof_xgb_clf >0.55] += 0.1\n",
    "\n",
    "y_pred[\"prediction\"] = pairwise_ranking_oof\n",
    "m = score(y_true.copy(), y_pred.copy(), \"ID\")\n",
    "print(f\"\\nPairwise ranking NN with classifier mask -> CV =\",m)\n",
    "\n",
    "pairwise_ranking_pred[pred_efs >0.45] += 0.1\n",
    "pairwise_ranking_pred[pred_efs >0.55] += 0.1\n",
    "subm_data = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "subm_data['prediction'] = pairwise_ranking_pred\n",
    "subm_data.to_csv('submission2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814cf31-1b39-4b91-894f-2a5af955e82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --requirement /kaggle/input/yunbase/Yunbase/requirements.txt  \\\n",
    "--no-index --find-links file:/kaggle/input/yunbase/\n",
    "\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install -q /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587d284-5449-46c2-b8db-baf30d4d7e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_file_path = '/kaggle/input/yunbase/Yunbase/baseline.py'\n",
    "target_file_path = '/kaggle/working/baseline.py'\n",
    "with open(source_file_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "with open(target_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c1575-3e88-4cda-bac1-7fc507588098",
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline import Yunbase\n",
    "import pandas as pd#read csv,parquet\n",
    "import numpy as np#for scientific computation of matrices\n",
    "from  lightgbm import LGBMRegressor,LGBMClassifier,log_evaluation,early_stopping\n",
    "from catboost import CatBoostRegressor,CatBoostClassifier\n",
    "from xgboost import XGBRegressor,XGBClassifier\n",
    "from lifelines import KaplanMeierFitter\n",
    "import warnings#avoid some negligible errors\n",
    "#The filterwarnings () method is used to set warning filters, which can control the output method and level of warning information.\n",
    "warnings.filterwarnings('ignore')\n",
    "import random#provide some function to generate random_seed.\n",
    "#set random seed,to make sure model can be recurrented.\n",
    "def seed_everything(seed):\n",
    "    np.random.seed(seed)#numpy's random seed\n",
    "    random.seed(seed)#python built-in random seed\n",
    "seed_everything(seed=2025)\n",
    "\n",
    "train=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "test=pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "train_solution=train[['ID','efs','efs_time','race_group']].copy()\n",
    "\n",
    "def logit(p):\n",
    "    return np.log(p) - np.log(1 - p)\n",
    "max_efs_time,min_efs_time=80,-100\n",
    "train['efs_time']=train['efs_time']/(max_efs_time-min_efs_time)\n",
    "train['efs_time']=train['efs_time'].apply(lambda x:logit(x))\n",
    "train['efs_time']+=10\n",
    "print(train['efs_time'].max(),train['efs_time'].min())\n",
    "\n",
    "race2weight={'American Indian or Alaska Native':0.68,\n",
    "'Asian':0.7,'Black or African-American':0.67,\n",
    "'More than one race':0.68,\n",
    "'Native Hawaiian or other Pacific Islander':0.66,\n",
    "'White':0.64}\n",
    "train['weight']=0.5*train['efs']+0.5\n",
    "train['raceweight']=train['race_group'].apply(lambda x:race2weight.get(x,1))\n",
    "train['weight']=train['weight']/train['raceweight']\n",
    "train.drop(['raceweight'],axis=1,inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad84775-ee92-4188-9aec-7e5e2477a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "\n",
    "    kmf = KaplanMeierFitter()\n",
    "    \n",
    "    kmf.fit(df[time_col], event_observed=df[event_col])\n",
    "    \n",
    "    survival_probabilities = kmf.survival_function_at_times(df[time_col]).values.flatten()\n",
    "\n",
    "    return survival_probabilities\n",
    "\n",
    "race_group=sorted(train['race_group'].unique())\n",
    "for race in race_group:\n",
    "    train.loc[train['race_group']==race,\"target\"] = transform_survival_probability(train[train['race_group']==race], time_col='efs_time', event_col='efs')\n",
    "    gap=0.7*(train.loc[(train['race_group']==race)&(train['efs']==0)]['target'].max()-train.loc[(train['race_group']==race)&(train['efs']==1)]['target'].min())/2\n",
    "    train.loc[(train['race_group']==race)&(train['efs']==0),'target']-=gap\n",
    "\n",
    "sns.histplot(data=train, x='target', hue='efs', element='step', stat='density', common_norm=False)\n",
    "plt.legend(title='efs')\n",
    "plt.title('Distribution of Target by EFS')\n",
    "plt.xlabel('Target')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "train.drop(['efs','efs_time'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec85099-d623-42a4-9b45-f791b95ff731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nunique=2\n",
    "nunique2=[col for col in train.columns if train[col].nunique()==2 and col!='efs']\n",
    "#nunique<50\n",
    "nunique50=[col for col in train.columns if train[col].nunique()<50 and col not in ['efs','weight']]+['age_group','dri_score_NA']\n",
    "\n",
    "def FE(df):\n",
    "    print(\"< deal with outlier >\")\n",
    "    df['nan_value_each_row'] = df.isnull().sum(axis=1)\n",
    "    #year_hct=2020 only 4 rows.\n",
    "    df['year_hct']=df['year_hct'].replace(2020,2019)\n",
    "    df['age_group']=df['age_at_hct']//10\n",
    "    #karnofsky_score 40 only 10 rows.\n",
    "    df['karnofsky_score']=df['karnofsky_score'].replace(40,50)\n",
    "    #hla_high_res_8=2 only 2 rows.\n",
    "    df['hla_high_res_8']=df['hla_high_res_8'].replace(2,3)\n",
    "    #hla_high_res_6=0 only 1 row.\n",
    "    df['hla_high_res_6']=df['hla_high_res_6'].replace(0,2)\n",
    "    #hla_high_res_10=3 only 1 row.\n",
    "    df['hla_high_res_10']=df['hla_high_res_10'].replace(3,4)\n",
    "    #hla_low_res_8=2 only 1 row.\n",
    "    df['hla_low_res_8']=df['hla_low_res_8'].replace(2,3)\n",
    "    df['dri_score']=df['dri_score'].replace('Missing disease status','N/A - disease not classifiable')\n",
    "    df['dri_score_NA']=df['dri_score'].apply(lambda x:int('N/A' in str(x)))\n",
    "    for col in ['diabetes','pulm_moderate','cardiac']:\n",
    "        df.loc[df[col].isna(),col]='Not done'\n",
    "\n",
    "    print(\"< cross feature >\")\n",
    "    df['donor_age-age_at_hct']=df['donor_age']-df['age_at_hct']\n",
    "    df['comorbidity_score+karnofsky_score']=df['comorbidity_score']+df['karnofsky_score']\n",
    "    df['comorbidity_score-karnofsky_score']=df['comorbidity_score']-df['karnofsky_score']\n",
    "    df['comorbidity_score*karnofsky_score']=df['comorbidity_score']*df['karnofsky_score']\n",
    "    df['comorbidity_score/karnofsky_score']=df['comorbidity_score']/df['karnofsky_score']\n",
    "    \n",
    "    print(\"< fillna >\")\n",
    "    df[nunique50]=df[nunique50].astype(str).fillna('NaN')\n",
    "    \n",
    "    print(\"< combine category feature >\")\n",
    "    for i in range(len(nunique2)):\n",
    "        for j in range(i+1,len(nunique2)):\n",
    "            df[nunique2[i]+nunique2[j]]=df[nunique2[i]].astype(str)+df[nunique2[j]].astype(str)\n",
    "    \n",
    "    print(\"< drop useless columns >\")\n",
    "    df.drop(['ID'],axis=1,inplace=True,errors='ignore')\n",
    "    return df\n",
    "\n",
    "combine_category_cols=[]\n",
    "for i in range(len(nunique2)):\n",
    "    for j in range(i+1,len(nunique2)):\n",
    "        combine_category_cols.append(nunique2[i]+nunique2[j])  \n",
    "\n",
    "total_category_feature=nunique50+combine_category_cols\n",
    "\n",
    "target_stat=[]\n",
    "for j in range(len(total_category_feature)):\n",
    "   for col in ['donor_age','age_at_hct','target']:\n",
    "    target_stat.append( (total_category_feature[j],col,['count','mean','max','std','skew']) )\n",
    "\n",
    "num_folds=10\n",
    "\n",
    "lgb_params={\"boosting_type\": \"gbdt\",\"metric\": 'mae',\n",
    "            'random_state': 2025,  \"max_depth\": 9,\"learning_rate\": 0.1,\n",
    "            \"n_estimators\": 768,\"colsample_bytree\": 0.6,\"colsample_bynode\": 0.6,\n",
    "            \"verbose\": -1,\"reg_alpha\": 0.2,\n",
    "            \"reg_lambda\": 5,\"extra_trees\":True,'num_leaves':64,\"max_bin\":255,\n",
    "            'importance_type': 'gain',#better than 'split'\n",
    "            'device':'gpu','gpu_use_dp':True\n",
    "           }\n",
    "\n",
    "cat_params={'random_state':2025,'eval_metric' : 'MAE',\n",
    "            'bagging_temperature': 0.50,'iterations': 650,\n",
    "            'learning_rate': 0.1,'max_depth': 8,\n",
    "            'l2_leaf_reg': 1.25,'min_data_in_leaf': 24,\n",
    "            'random_strength' : 0.25, 'verbose': 0,\n",
    "            'task_type':'GPU',\n",
    "            }\n",
    "xgb_params={'random_state': 2025, 'n_estimators': 256, \n",
    "            'learning_rate': 0.1, 'max_depth': 6,\n",
    "            'reg_alpha': 0.08, 'reg_lambda': 0.8, \n",
    "            'subsample': 0.95, 'colsample_bytree': 0.6, \n",
    "            'min_child_weight': 3,'early_stopping_rounds':1024,\n",
    "             'enable_categorical':True,'tree_method':'gpu_hist'\n",
    "            }\n",
    "\n",
    "yunbase=Yunbase(num_folds=num_folds,\n",
    "                  models=[(LGBMRegressor(**lgb_params),'lgb'),\n",
    "                          (CatBoostRegressor(**cat_params),'cat'),\n",
    "                          # (XGBRegressor(**xgb_params),'xgb')\n",
    "                         ],\n",
    "                  FE=FE,\n",
    "                  seed=2025,\n",
    "                  objective='regression',\n",
    "                  metric='mae',\n",
    "                  target_col='target',\n",
    "                  device='gpu',\n",
    "                  one_hot_max=-1,\n",
    "                  early_stop=1000,\n",
    "                  cross_cols=['donor_age','age_at_hct'],\n",
    "                  target_stat=target_stat,\n",
    "                  use_data_augmentation=True,\n",
    "                  use_scaler=True,\n",
    "                  log=250,\n",
    "                  plot_feature_importance=True,\n",
    "                  #print metric score when model training\n",
    "                  use_eval_metric=False,\n",
    ")\n",
    "yunbase.fit(train,category_cols=nunique2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d622ef-dc6f-4b1f-8f28-2a1526e59feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas.api.types\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2a44d-3546-4f26-9739-ac2cc3bf9761",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str) -> float:\n",
    "    \n",
    "    del solution[row_id_column_name]\n",
    "    del submission[row_id_column_name]\n",
    "    \n",
    "    event_label = 'efs'\n",
    "    interval_label = 'efs_time'\n",
    "    prediction_label = 'prediction'\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "    # Merging solution and submission dfs on ID\n",
    "    merged_df = pd.concat([solution, submission], axis=1)\n",
    "    merged_df.reset_index(inplace=True)\n",
    "    merged_df_race_dict = dict(merged_df.groupby(['race_group']).groups)\n",
    "    metric_list = []\n",
    "    for race in merged_df_race_dict.keys():\n",
    "        # Retrieving values from y_test based on index\n",
    "        indices = sorted(merged_df_race_dict[race])\n",
    "        merged_df_race = merged_df.iloc[indices]\n",
    "        # Calculate the concordance index\n",
    "        c_index_race = concordance_index(\n",
    "                        merged_df_race[interval_label],\n",
    "                        -merged_df_race[prediction_label],\n",
    "                        merged_df_race[event_label])\n",
    "        metric_list.append(c_index_race)\n",
    "    return float(np.mean(metric_list)-np.sqrt(np.var(metric_list)))\n",
    "\n",
    "weights = [0.602,0.398]\n",
    "test_preds=yunbase.predict(test,weights=weights)\n",
    "yunbase.target_col='prediction'\n",
    "yunbase.submit(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\",test_preds,\n",
    "               save_name='submission1'\n",
    "              )\n",
    "pd.read_csv('submission1.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93301ed1-1fb3-4341-9785-f7779389768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install /kaggle/input/pip-install-lifelines/autograd-1.7.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/autograd-gamma-0.5.0.tar.gz\n",
    "!pip install /kaggle/input/pip-install-lifelines/interface_meta-1.3.0-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/formulaic-1.0.2-py3-none-any.whl\n",
    "!pip install /kaggle/input/pip-install-lifelines/lifelines-0.30.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ba3f5-a54f-4184-9788-2f9e1158750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import plotly.colors as pc\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import NelsonAalenFitter\n",
    "import lightgbm as lgb\n",
    "from metric import score\n",
    "from scipy.stats import rankdata \n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "class CFG:\n",
    "\n",
    "    train_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "    test_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "    subm_path = Path('/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv')\n",
    "    \n",
    "    colorscale = 'Redor'\n",
    "    color = '#A2574F'\n",
    "\n",
    "    batch_size = 32768\n",
    "    early_stop = 300\n",
    "    penalizer = 0.01\n",
    "    n_splits = 5\n",
    "\n",
    "    weights = [2, 1, 6, 3, 6, 3, 6, 6]\n",
    "\n",
    "    ctb_params = {\n",
    "        'loss_function': 'RMSE',\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'num_trees': 6000,\n",
    "        'reg_lambda': 8.0,\n",
    "        'depth': 8\n",
    "    }\n",
    "\n",
    "    lgb_params = {\n",
    "        'objective': 'regression',\n",
    "        'min_child_samples': 32,\n",
    "        'num_iterations': 6000,\n",
    "        'learning_rate': 0.03,\n",
    "        'extra_trees': True,\n",
    "        'reg_lambda': 8.0,\n",
    "        'reg_alpha': 0.1,\n",
    "        'num_leaves': 64,\n",
    "        'metric': 'rmse',\n",
    "        'max_depth': 8,\n",
    "        'device': 'cpu',\n",
    "        'max_bin': 128,\n",
    "        'verbose': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    cox1_params = {\n",
    "        'grow_policy': 'Depthwise',\n",
    "        'min_child_samples': 8,\n",
    "        'loss_function': 'Cox',\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'num_trees': 6000,\n",
    "        'reg_lambda': 8.0,\n",
    "        'depth': 8\n",
    "    }\n",
    "\n",
    "    cox2_params = {\n",
    "        'grow_policy': 'Lossguide',\n",
    "        'min_child_samples': 2,\n",
    "        'loss_function': 'Cox',\n",
    "        'learning_rate': 0.03,\n",
    "        'random_state': 42,\n",
    "        'task_type': 'CPU',\n",
    "        'num_trees': 6000,\n",
    "        'reg_lambda': 8.0,\n",
    "        'num_leaves': 32,\n",
    "        'depth': 8\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3ac2e-905f-4284-a565-c73d0c3fe691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE:\n",
    "\n",
    "    def __init__(self, batch_size):\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def _load_data(self, path):\n",
    "\n",
    "        return pl.read_csv(path, batch_size=self._batch_size)\n",
    "\n",
    "    def _update_hla_columns(self, df):\n",
    "        \n",
    "        df = df.with_columns(\n",
    "            \n",
    "            pl.col('hla_match_a_low').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_high').fill_null(0))\n",
    "            .alias('hla_nmdp_6'),\n",
    "            \n",
    "            pl.col('hla_match_a_low').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_low').fill_null(0))\n",
    "            .alias('hla_low_res_6'),\n",
    "            \n",
    "            pl.col('hla_match_a_high').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_high').fill_null(0))\n",
    "            .alias('hla_high_res_6'),\n",
    "            \n",
    "            pl.col('hla_match_a_low').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_c_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_low').fill_null(0))\n",
    "            .alias('hla_low_res_8'),\n",
    "            \n",
    "            pl.col('hla_match_a_high').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_c_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_high').fill_null(0))\n",
    "            .alias('hla_high_res_8'),\n",
    "            \n",
    "            pl.col('hla_match_a_low').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_c_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_low').fill_null(0))\n",
    "            .add(pl.col('hla_match_dqb1_low').fill_null(0))\n",
    "            .alias('hla_low_res_10'),\n",
    "            \n",
    "            pl.col('hla_match_a_high').fill_null(0)\n",
    "            .add(pl.col('hla_match_b_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_c_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_drb1_high').fill_null(0))\n",
    "            .add(pl.col('hla_match_dqb1_high').fill_null(0))\n",
    "            .alias('hla_high_res_10'),\n",
    "            \n",
    "        )\n",
    "    \n",
    "        return df\n",
    "\n",
    "    def _cast_datatypes(self, df):\n",
    "\n",
    "        num_cols = [\n",
    "            'hla_high_res_8',\n",
    "            'hla_low_res_8',\n",
    "            'hla_high_res_6',\n",
    "            'hla_low_res_6',\n",
    "            'hla_high_res_10',\n",
    "            'hla_low_res_10',\n",
    "            'hla_match_dqb1_high',\n",
    "            'hla_match_dqb1_low',\n",
    "            'hla_match_drb1_high',\n",
    "            'hla_match_drb1_low',\n",
    "            'hla_nmdp_6',\n",
    "            'year_hct',\n",
    "            'hla_match_a_high',\n",
    "            'hla_match_a_low',\n",
    "            'hla_match_b_high',\n",
    "            'hla_match_b_low',\n",
    "            'hla_match_c_high',\n",
    "            'hla_match_c_low',\n",
    "            'donor_age',\n",
    "            'age_at_hct',\n",
    "            'comorbidity_score',\n",
    "            'karnofsky_score',\n",
    "            'efs',\n",
    "            'efs_time'\n",
    "        ]\n",
    "\n",
    "        for col in df.columns:\n",
    "\n",
    "            if col in num_cols:\n",
    "                df = df.with_columns(pl.col(col).fill_null(-1).cast(pl.Float32))  \n",
    "\n",
    "            else:\n",
    "                df = df.with_columns(pl.col(col).fill_null('Unknown').cast(pl.String))  \n",
    "\n",
    "        return df.with_columns(pl.col('ID').cast(pl.Int32))\n",
    "\n",
    "    def info(self, df):\n",
    "        \n",
    "        print(f'\\nShape of dataframe: {df.shape}') \n",
    "        \n",
    "        mem = df.memory_usage().sum() / 1024**2\n",
    "        print('Memory usage: {:.2f} MB\\n'.format(mem))\n",
    "\n",
    "        display(df.head())\n",
    "\n",
    "    def apply_fe(self, path):\n",
    "\n",
    "        df = self._load_data(path)   \n",
    "        df = self._update_hla_columns(df)                     \n",
    "        df = self._cast_datatypes(df)        \n",
    "        df = df.to_pandas()\n",
    "        self.info(df)\n",
    "        \n",
    "        cat_cols = [col for col in df.columns if df[col].dtype == pl.String]\n",
    "\n",
    "        return df, cat_cols\n",
    "\n",
    "fe = FE(CFG.batch_size)\n",
    "train_data, cat_cols = fe.apply_fe(CFG.train_path)\n",
    "test_data, _ = fe.apply_fe(CFG.test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ece7fe-f461-4389-81e0-920d0470c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    \n",
    "    def __init__(self, colorscale, color, data):\n",
    "        self._colorscale = colorscale\n",
    "        self._color = color  \n",
    "        self.data = data\n",
    "\n",
    "    def _template(self, fig, title):\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=title,\n",
    "            title_x=0.5, \n",
    "            plot_bgcolor='rgba(247, 230, 202, 1)',  \n",
    "            paper_bgcolor='rgba(247, 230, 202, 1)', \n",
    "            font=dict(color=self._color),\n",
    "            margin=dict(l=72, r=72, t=72, b=72), \n",
    "            height=720\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "    def distribution_plot(self, col, title):\n",
    "        \n",
    "        fig = px.histogram(\n",
    "            self.data,\n",
    "            x=col,\n",
    "            nbins=100,\n",
    "            color_discrete_sequence=[self._color]\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title='Values',\n",
    "            yaxis_title='Count',\n",
    "            bargap=0.1,\n",
    "            xaxis=dict(gridcolor='grey'),\n",
    "            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n",
    "        )\n",
    "        \n",
    "        fig.update_traces(hovertemplate='Value: %{x:.2f}<br>Count: %{y:,}')\n",
    "        \n",
    "        fig = self._template(fig, f'{title}')\n",
    "        fig.show()\n",
    "    \n",
    "    def bar_chart(self, col):\n",
    "        \n",
    "        value_counts = self.data[col].value_counts().reset_index()\n",
    "        value_counts.columns = [col, 'count']\n",
    "        \n",
    "        fig = px.bar(\n",
    "            value_counts,\n",
    "            y=col,\n",
    "            x='count',\n",
    "            orientation='h',\n",
    "            color='count',\n",
    "            color_continuous_scale=self._colorscale,\n",
    "        )\n",
    "        \n",
    "        fig.update_layout(\n",
    "            xaxis_title='Count',\n",
    "            yaxis_title='',\n",
    "            xaxis=dict(gridcolor='grey'),\n",
    "            yaxis=dict(gridcolor='grey', zerolinecolor='grey')\n",
    "        )\n",
    "        \n",
    "        fig.update_traces(\n",
    "            hovertemplate=(\n",
    "                f'<b>{col}:</b> %{{y}}<br>'\n",
    "                '<b>Count:</b> %{x:,}<br>'\n",
    "            ),\n",
    "            hoverlabel=dict(\n",
    "                font=dict(color=self._color),\n",
    "                bgcolor='rgba(247, 230, 202, 1)'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig = self._template(fig, f'{col}')\n",
    "        fig.show()\n",
    "        \n",
    "    def _plot_cv(self, scores, title, metric='Stratified C-Index'):\n",
    "        \n",
    "        fold_scores = [round(score, 3) for score in scores]\n",
    "        mean_score = round(np.mean(scores), 3)\n",
    "\n",
    "        fig = go.Figure()\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = list(range(1, len(fold_scores) + 1)),\n",
    "            y = fold_scores,\n",
    "            mode = 'markers', \n",
    "            name = 'Fold Scores',\n",
    "            marker = dict(size = 27, color=self._color, symbol='diamond'),\n",
    "            text = [f'{score:.3f}' for score in fold_scores],\n",
    "            hovertemplate = 'Fold %{x}: %{text}<extra></extra>',\n",
    "            hoverlabel = dict(font=dict(size=18))  \n",
    "        ))\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x = [1, len(fold_scores)],\n",
    "            y = [mean_score, mean_score],\n",
    "            mode = 'lines',\n",
    "            name = f'Mean: {mean_score:.3f}',\n",
    "            line = dict(dash = 'dash', color = '#B22222'),\n",
    "            hoverinfo = 'none'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title = f'{title} | Cross-validation Mean {metric} Score: {mean_score}',\n",
    "            xaxis_title = 'Fold',\n",
    "            yaxis_title = f'{metric} Score',\n",
    "            plot_bgcolor = 'rgba(247, 230, 202, 1)',  \n",
    "            paper_bgcolor = 'rgba(247, 230, 202, 1)',\n",
    "            font = dict(color=self._color), \n",
    "            xaxis = dict(\n",
    "                gridcolor = 'grey',\n",
    "                tickmode = 'linear',\n",
    "                tick0 = 1,\n",
    "                dtick = 1,\n",
    "                range = [0.5, len(fold_scores) + 0.5],\n",
    "                zerolinecolor = 'grey'\n",
    "            ),\n",
    "            yaxis = dict(\n",
    "                gridcolor = 'grey',\n",
    "                zerolinecolor = 'grey'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        fig.show()\n",
    "\n",
    "class Targets:\n",
    "\n",
    "    def __init__(self, data, cat_cols, penalizer, n_splits):\n",
    "        \n",
    "        self.data = data\n",
    "        self.cat_cols = cat_cols\n",
    "        \n",
    "        self._length = len(self.data)\n",
    "        self._penalizer = penalizer\n",
    "        self._n_splits = n_splits\n",
    "\n",
    "    def _prepare_cv(self):\n",
    "        \n",
    "        oof_preds = np.zeros(self._length)\n",
    "            \n",
    "        cv = KFold(n_splits=self._n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "        return cv, oof_preds\n",
    "\n",
    "    def validate_model(self, preds, title):\n",
    "            \n",
    "        y_true = self.data[['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "        y_pred = self.data[['ID']].copy()\n",
    "        \n",
    "        y_pred['prediction'] = preds\n",
    "            \n",
    "        c_index_score = score(y_true.copy(), y_pred.copy(), 'ID')\n",
    "        print(f'Overall Stratified C-Index Score for {title}: {c_index_score:.4f}')\n",
    "\n",
    "    def create_target1(self):  \n",
    "\n",
    "        '''\n",
    "        Constant columns are dropped if they exist in a fold. Otherwise, the code produces error:\n",
    "\n",
    "        delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: \n",
    "        https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n",
    "        '''\n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        # Apply one hot encoding to categorical columns\n",
    "        data = pd.get_dummies(self.data, columns=self.cat_cols, drop_first=True).drop('ID', axis=1) \n",
    "\n",
    "        for train_index, valid_index in cv.split(data):\n",
    "\n",
    "            train_data = data.iloc[train_index]\n",
    "            valid_data = data.iloc[valid_index]\n",
    "\n",
    "            # Drop constant columns if they exist\n",
    "            train_data = train_data.loc[:, train_data.nunique() > 1]\n",
    "            valid_data = valid_data[train_data.columns]\n",
    "\n",
    "            cph = CoxPHFitter(penalizer=self._penalizer)\n",
    "            cph.fit(train_data, duration_col='efs_time', event_col='efs')\n",
    "            \n",
    "            oof_preds[valid_index] = cph.predict_partial_hazard(valid_data)              \n",
    "\n",
    "        self.data['target1'] = oof_preds \n",
    "        self.validate_model(oof_preds, 'Cox') \n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target2(self):        \n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "            train_data = self.data.iloc[train_index]\n",
    "            valid_data = self.data.iloc[valid_index]\n",
    "\n",
    "            kmf = KaplanMeierFitter()\n",
    "            kmf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "            \n",
    "            oof_preds[valid_index] = kmf.survival_function_at_times(valid_data['efs_time']).values\n",
    "\n",
    "        self.data['target2'] = oof_preds  \n",
    "        self.validate_model(oof_preds, 'Kaplan-Meier')\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target3(self):        \n",
    "\n",
    "        cv, oof_preds = self._prepare_cv()\n",
    "\n",
    "        for train_index, valid_index in cv.split(self.data):\n",
    "\n",
    "            train_data = self.data.iloc[train_index]\n",
    "            valid_data = self.data.iloc[valid_index]\n",
    "            \n",
    "            naf = NelsonAalenFitter()\n",
    "            naf.fit(durations=train_data['efs_time'], event_observed=train_data['efs'])\n",
    "            \n",
    "            oof_preds[valid_index] = -naf.cumulative_hazard_at_times(valid_data['efs_time']).values\n",
    "\n",
    "        self.data['target3'] = oof_preds  \n",
    "        self.validate_model(oof_preds, 'Nelson-Aalen')\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def create_target4(self):\n",
    "\n",
    "        self.data['target4'] = self.data.efs_time.copy()\n",
    "        self.data.loc[self.data.efs == 0, 'target4'] *= -1\n",
    "\n",
    "        return self.data\n",
    "\n",
    "class MD:\n",
    "    \n",
    "    def __init__(self, colorscale, color, data, cat_cols, early_stop, penalizer, n_splits):\n",
    "        \n",
    "        self.eda = EDA(colorscale, color, data)\n",
    "        self.targets = Targets(data, cat_cols, penalizer, n_splits)\n",
    "        \n",
    "        self.data = data\n",
    "        self.cat_cols = cat_cols\n",
    "        self._early_stop = early_stop\n",
    "\n",
    "    def create_targets(self):\n",
    "\n",
    "        self.data = self.targets.create_target1()\n",
    "        self.data = self.targets.create_target2()\n",
    "        self.data = self.targets.create_target3()\n",
    "        self.data = self.targets.create_target4()\n",
    "\n",
    "        return self.data\n",
    "        \n",
    "    def train_model(self, params, target, title):\n",
    "        \n",
    "        for col in self.cat_cols:\n",
    "            self.data[col] = self.data[col].astype('category')\n",
    "            \n",
    "        X = self.data.drop(['ID', 'efs', 'efs_time', 'target1', 'target2', 'target3', 'target4'], axis=1)\n",
    "        y = self.data[target]\n",
    "        \n",
    "        models, fold_scores = [], []\n",
    "            \n",
    "        cv, oof_preds = self.targets._prepare_cv()\n",
    "    \n",
    "        for fold, (train_index, valid_index) in enumerate(cv.split(X, y)):\n",
    "                \n",
    "            X_train = X.iloc[train_index]\n",
    "            X_valid = X.iloc[valid_index]\n",
    "                \n",
    "            y_train = y.iloc[train_index]\n",
    "            y_valid = y.iloc[valid_index]\n",
    "    \n",
    "            if title.startswith('LightGBM'):\n",
    "                        \n",
    "                model = lgb.LGBMRegressor(**params)\n",
    "                        \n",
    "                model.fit(\n",
    "                    X_train, \n",
    "                    y_train,  \n",
    "                    eval_set=[(X_valid, y_valid)],\n",
    "                    eval_metric='rmse',\n",
    "                    callbacks=[lgb.early_stopping(self._early_stop, verbose=0), lgb.log_evaluation(0)]\n",
    "                )\n",
    "                        \n",
    "            elif title.startswith('CatBoost'):\n",
    "                        \n",
    "                model = CatBoostRegressor(**params, verbose=0, cat_features=self.cat_cols)\n",
    "                        \n",
    "                model.fit(\n",
    "                    X_train,\n",
    "                    y_train,\n",
    "                    eval_set=(X_valid, y_valid),\n",
    "                    early_stopping_rounds=self._early_stop, \n",
    "                    verbose=0\n",
    "                )               \n",
    "                    \n",
    "            models.append(model)\n",
    "                \n",
    "            oof_preds[valid_index] = model.predict(X_valid)\n",
    "\n",
    "            y_true_fold = self.data.iloc[valid_index][['ID', 'efs', 'efs_time', 'race_group']].copy()\n",
    "            y_pred_fold = self.data.iloc[valid_index][['ID']].copy()\n",
    "            \n",
    "            y_pred_fold['prediction'] = oof_preds[valid_index]\n",
    "    \n",
    "            fold_score = score(y_true_fold, y_pred_fold, 'ID')\n",
    "            fold_scores.append(fold_score)\n",
    "    \n",
    "        self.eda._plot_cv(fold_scores, title)\n",
    "        self.targets.validate_model(oof_preds, title)\n",
    "        \n",
    "        return models, oof_preds\n",
    "\n",
    "    def infer_model(self, data, models):\n",
    "        \n",
    "        data = data.drop(['ID'], axis=1)\n",
    "\n",
    "        for col in self.cat_cols:\n",
    "            data[col] = data[col].astype('category')\n",
    "\n",
    "        return np.mean([model.predict(data) for model in models], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98036cb7-d99a-49f3-a473-7cde6e572fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "md = MD(CFG.colorscale, CFG.color, train_data, cat_cols, CFG.early_stop, CFG.penalizer, CFG.n_splits)\n",
    "train_data = md.create_targets()\n",
    "fe.info(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23d2513-7065-49c3-a095-06db1b5b3b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb1_models, ctb1_oof_preds = md.train_model(CFG.ctb_params, target='target1', title='CatBoost')\n",
    "lgb1_models, lgb1_oof_preds = md.train_model(CFG.lgb_params, target='target1', title='LightGBM')\n",
    "ctb2_models, ctb2_oof_preds = md.train_model(CFG.ctb_params, target='target2', title='CatBoost')\n",
    "lgb2_models, lgb2_oof_preds = md.train_model(CFG.lgb_params, target='target2', title='LightGBM')\n",
    "ctb3_models, ctb3_oof_preds = md.train_model(CFG.ctb_params, target='target3', title='CatBoost')\n",
    "lgb3_models, lgb3_oof_preds = md.train_model(CFG.lgb_params, target='target3', title='LightGBM')\n",
    "cox1_models, cox1_oof_preds = md.train_model(CFG.cox1_params, target='target4', title='CatBoost')\n",
    "cox2_models, cox2_oof_preds = md.train_model(CFG.cox2_params, target='target4', title='CatBoost')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d796829f-d22e-434e-92f7-78da958b7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctb1_preds = md.infer_model(test_data, ctb1_models)\n",
    "lgb1_preds = md.infer_model(test_data, lgb1_models)\n",
    "ctb2_preds = md.infer_model(test_data, ctb2_models)\n",
    "lgb2_preds = md.infer_model(test_data, lgb2_models)\n",
    "ctb3_preds = md.infer_model(test_data, ctb3_models)\n",
    "lgb3_preds = md.infer_model(test_data, lgb3_models)\n",
    "cox1_preds = md.infer_model(test_data, cox1_models)\n",
    "cox2_preds = md.infer_model(test_data, cox2_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f959c80-2600-4994-8356-a2095285b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = [\n",
    "    ctb1_oof_preds, \n",
    "    lgb1_oof_preds, \n",
    "    ctb2_oof_preds, \n",
    "    lgb2_oof_preds, \n",
    "    ctb3_oof_preds, \n",
    "    lgb3_oof_preds, \n",
    "    cox1_oof_preds,\n",
    "    cox2_oof_preds\n",
    "]\n",
    "\n",
    "preds = [\n",
    "    ctb1_preds, \n",
    "    lgb1_preds, \n",
    "    ctb2_preds, \n",
    "    lgb2_preds, \n",
    "    ctb3_preds, \n",
    "    lgb3_preds,\n",
    "    cox1_preds,\n",
    "    cox2_preds\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71b77d-0077-4f61-b138-db49aeb09876",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_oof_preds = np.array([rankdata(p) for p in oof_preds])\n",
    "ensemble_oof_preds = np.dot(CFG.weights, ranked_oof_preds)\n",
    "md.targets.validate_model(ensemble_oof_preds, 'Ensemble Model')\n",
    "ranked_preds = np.array([rankdata(p) for p in preds])\n",
    "ensemble_preds = np.dot(CFG.weights, ranked_preds)\n",
    "subm_data = pd.read_csv(CFG.subm_path)\n",
    "subm_data['prediction'] = ensemble_preds\n",
    "subm_data.to_csv('submission3.csv', index=False)\n",
    "display(subm_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0da7e0-5946-46ac-bd70-a31c9aa248dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def add_features(df):\n",
    "    sex_match = df.sex_match.astype(str)\n",
    "    sex_match = sex_match.str.split(\"-\").str[0] == sex_match.str.split(\"-\").str[1]\n",
    "    df['sex_match_bool'] = sex_match.astype(\"object\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "\n",
    "def transform_survival_probability(df, time_col='efs_time', event_col='efs'):\n",
    "    kmf = KaplanMeierFitter()\n",
    "    df_ = df.loc[(df[time_col] < 24) | (df[event_col] == 0)]\n",
    "    kmf.fit(df_[time_col], df_[event_col])\n",
    "    y = kmf.survival_function_at_times(df[time_col]).values\n",
    "    plt.hist(df.efs_time[df[event_col] == 0], bins=150, alpha=0.5, label=\"Event\")\n",
    "    plt.show()\n",
    "    #y = y / (y.max() - y.min())\n",
    "    # y = np.log(1 + np.log(y / (1 - y)))\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "from metric import score as score_f\n",
    "\n",
    "\n",
    "\n",
    "def run(hparams=None):\n",
    "    FOLDS, oof_xgb, pred_xgb, train = make_predictions(hparams)\n",
    "\n",
    "    # COMPUTE AVERAGE TEST PREDS\n",
    "    pred_xgb /= FOLDS\n",
    "\n",
    "    y_true = train[[\"ID\", \"efs\", \"efs_time\", \"race_group\"]].copy()\n",
    "    y_pred = train[[\"ID\"]].copy()\n",
    "    y_pred[\"prediction\"] = oof_xgb\n",
    "    m = score_f(train.copy(), y_pred.copy(), \"ID\")\n",
    "    plt.hist(oof_xgb[train.efs==1], bins=np.linspace(-3, 3, 200), alpha=0.5, label=\"Event\")\n",
    "    plt.hist(oof_xgb[train.efs==0], bins=np.linspace(-3, 3, 200), alpha=0.5, label=\"No event\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(f\"\\nOverall CV for XGBoost KaplanMeier =\", m)\n",
    "    return pred_xgb,oof_xgb\n",
    "\n",
    "\n",
    "def make_predictions(hparams):\n",
    "    if hparams is None:\n",
    "        hparams = dict(\n",
    "            max_depth=6,\n",
    "            colsample_bytree=0.5,\n",
    "            subsample=0.8,\n",
    "            n_estimators=3000,\n",
    "            learning_rate=0.01,\n",
    "            min_child_weight=40,\n",
    "            gamma=1,\n",
    "            eta=0.0,\n",
    "            reg_lambda=0.1,\n",
    "            reg_alpha=0.1,\n",
    "            eps=2e-2,\n",
    "            eps_mul=1.01,\n",
    "            pos_shift=0.2\n",
    "        )\n",
    "    FEATURES, test, train, y = prepare_data(hparams.pop('eps'), hparams.pop('eps_mul'))\n",
    "    FOLDS = 5\n",
    "    kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "    oof_xgb = np.zeros(len(train))\n",
    "    pred_xgb = np.zeros(len(test))\n",
    "    pos_shift = hparams.pop('pos_shift')\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(train, train.race_group)):\n",
    "        print(\"#\" * 25)\n",
    "        print(f\"### Fold {i + 1}\")\n",
    "        print(\"#\" * 25)\n",
    "        x_test, x_train, x_valid, y_train, y_valid = _prepare_training_data(\n",
    "            FEATURES, test.copy(), test_index, train.copy(), train_index, y.copy(), pos_shift=pos_shift\n",
    "        )\n",
    "        hparams.update(\n",
    "            dict(\n",
    "                objective='reg:pseudohubererror',\n",
    "                max_cat_to_onehot=10,\n",
    "                device=\"cuda\",\n",
    "                enable_categorical=True,\n",
    "                random_state=42,\n",
    "                monotone_constraints={\n",
    "                    # 'comorbidity_score': 1,\n",
    "                    # 'hla_match_c_high': -1,\n",
    "                    # 'hla_high_res_10': -1,\n",
    "                    'hla_high_res_6': -1,\n",
    "                    'hla_high_res_8': -1,\n",
    "                    # 'hla_low_res_10': -1,\n",
    "                    'hla_low_res_6': -1,\n",
    "                    # 'hla_low_res_8': -1,\n",
    "                    'hla_match_a_high': -1,\n",
    "                    # 'hla_match_a_low': -1,\n",
    "                    # 'hla_match_b_high': -1,\n",
    "                    'hla_match_drb1_low': -1,\n",
    "                    'hla_match_c_low': -1,\n",
    "                    # 'hla_match_c_high': -1,\n",
    "                    # 'donor_age': -1,\n",
    "                    # 'hla_match_drb1_high': -1,\n",
    "                    'hla_match_dqb1_low': -1,\n",
    "                    'hla_nmdp_6': -1,\n",
    "                    # 'karnofsky_score': -1,\n",
    "\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "        print(hparams['n_estimators'])\n",
    "        model_xgb = XGBRegressor(\n",
    "            **hparams\n",
    "        )\n",
    "        tt = train.loc[train_index]\n",
    "\n",
    "        array = np.array([.95 if x else 1 for x in ((tt.efs_time < 24) & (tt.efs == 0)).values])\n",
    "        array /= np.array([1.3 if x else 1 for x in tt.efs_time > 36])\n",
    "        model_xgb.fit(\n",
    "            x_train, y_train,\n",
    "            eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "            verbose=100,\n",
    "            sample_weight=array\n",
    "        )\n",
    "\n",
    "        # INFER OOF\n",
    "        oof_xgb[test_index] = model_xgb.predict(x_valid)\n",
    "        # INFER TEST\n",
    "        pred_xgb += model_xgb.predict(x_test)\n",
    "    return FOLDS, oof_xgb, pred_xgb, train\n",
    "\n",
    "\n",
    "def _prepare_training_data(FEATURES, test, test_index, train, train_index, y, pos_shift=0.1):\n",
    "    y[train.efs == 0] = y[train.efs == 1].min() - pos_shift\n",
    "    std = np.std(y[train_index])\n",
    "    x_train = train.loc[train_index, FEATURES].copy()\n",
    "    y_train = y[train_index] / std\n",
    "    x_valid = train.loc[test_index, FEATURES].copy()\n",
    "    y_valid = y[test_index] / std\n",
    "    x_test = test[FEATURES].copy()\n",
    "    # ind = (train.loc[train_index].efs_time < 24) | (train.loc[train_index].efs == 0)\n",
    "    # x_train = x_train[ind]\n",
    "    # y_train = y_train[ind]\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    val = train.loc[test_index]\n",
    "    return x_test, x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(eps=2e-2, eps_mul=1.1):\n",
    "    test = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/test.csv\")\n",
    "    test = add_features(test)\n",
    "    train = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/train.csv\")\n",
    "    train = add_features(train)\n",
    "    train[\"y\"] = transform_survival_probability(train, time_col='efs_time', event_col='efs')\n",
    "    RMV = [\"ID\", \"efs\", \"efs_time\", \"y\"]\n",
    "    FEATURES = [c for c in train.columns if not c in RMV]\n",
    "\n",
    "    CATS = []\n",
    "    for c in FEATURES:\n",
    "        if train[c].dtype == \"object\":\n",
    "            CATS.append(c)\n",
    "            train[c] = train[c].fillna(\"NAN\")\n",
    "            test[c] = test[c].fillna(\"NAN\")\n",
    "    print(f\"In these features, there are {len(CATS)} CATEGORICAL FEATURES: {CATS}\")\n",
    "    print(f\"In these features, there are {len(CATS)} NUMERICAL FEATURES: {sorted([f for f in FEATURES if f not in CATS])}\")\n",
    "    combined = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "    # print(\"Combined data shape:\", combined.shape )\n",
    "    # LABEL ENCODE CATEGORICAL FEATURES\n",
    "    for c in FEATURES:\n",
    "\n",
    "        # LABEL ENCODE CATEGORICAL AND CONVERT TO INT32 CATEGORY\n",
    "        if c in CATS:\n",
    "            combined[c], _ = combined[c].factorize()\n",
    "            combined[c] -= combined[c].min()\n",
    "            combined[c] = combined[c].astype(\"int32\")\n",
    "            combined[c] = combined[c].astype(\"category\")\n",
    "\n",
    "        # REDUCE PRECISION OF NUMERICAL TO 32BIT TO SAVE MEMORY\n",
    "        else:\n",
    "            if combined[c].dtype == \"float64\":\n",
    "                combined[c] = combined[c].astype(\"float32\")\n",
    "            if combined[c].dtype == \"int64\":\n",
    "                combined[c] = combined[c].astype(\"int32\")\n",
    "    train = combined.iloc[:len(train)].copy()\n",
    "    test = combined.iloc[len(train):].reset_index(drop=True).copy()\n",
    "    y = train.y.copy()\n",
    "    # y = np.log(y)\n",
    "    y = (y - y.min() + eps) / (y.max() - y.min() + eps_mul * eps)\n",
    "    y = np.log(y / (1 - y))\n",
    "    plt.hist(y[train.efs == 1], bins=np.linspace(-4, 8, 150), label=\"Event\")\n",
    "    plt.hist(y[train.efs == 0], bins=np.linspace(-4, 8, 150), label=\"No event\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return FEATURES, test, train, y\n",
    "\n",
    "pred_xgb,oof_xgb = run(None)\n",
    "sub = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "sub.prediction = pred_xgb\n",
    "sub.to_csv(\"submission4.csv\",index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec428e0-33d8-4043-91fc-fd6704d1d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "sub1 = pd.read_csv('/kaggle/working/submission1.csv')\n",
    "sub2 = pd.read_csv('/kaggle/working/submission2.csv')\n",
    "sub3 = pd.read_csv('/kaggle/working/submission3.csv')\n",
    "sub4 = pd.read_csv('/kaggle/working/submission4.csv')\n",
    "ss1 = sub1.prediction.values\n",
    "ss2 = sub2.prediction.values\n",
    "ss3 = sub3.prediction.values\n",
    "ss4 = sub4.prediction.values\n",
    "ss1[pred_efs > 0.5] += 0.2\n",
    "ss4[pred_efs > 0.5] += 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98be49a4-ce8c-4541-98f6-772feb2d8702",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/equity-post-HCT-survival-predictions/sample_submission.csv\")\n",
    "sub.prediction = rankdata(ss1)*0.5+rankdata(ss2)*0.6+rankdata(ss3)*0.4+rankdata(ss4)*0.3\n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "print(\"Sub shape:\",sub.shape)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5839e74-7f06-46f2-8862-5c75d71f3f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219b861-c9d5-4151-92b1-ff64e7c2e0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
